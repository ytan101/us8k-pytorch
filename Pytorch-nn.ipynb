{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a1c847bcbf53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msetuptools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLooseVersion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os.path\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist_datasets():\n",
    "    train_data = datasets.MNIST(\n",
    "        root=\"data\", download=True, train=True, transform=ToTensor()\n",
    "    )\n",
    "    validation_data = datasets.MNIST(\n",
    "        root=\"data\", download=True, train=False, transform=ToTensor()\n",
    "    )\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset downloaded\n"
     ]
    }
   ],
   "source": [
    "train_data, _ = download_mnist_datasets()\n",
    "print(\"MNIST dataset downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_data_loader = DataLoader(train_data, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForwardNet inherits from nn.Module\n",
    "class FeedForwardNet(nn.Module):\n",
    "    \n",
    "    # Constructor for defining the different layers\n",
    "    def __init__(self):\n",
    "        # Super allows you to call functions from nn.Module directly due to inheritance\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reshape into 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Dense layers\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                28 * 28, 256\n",
    "            ),  # 28*28 inputs (from image) and 256 outputs (neurons)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),  # 256 inputs and 10 outputs (MNIST has 10 classes)\n",
    "        )\n",
    "\n",
    "        # Normalizes output from 0 to 1 (probability of it being that class)\n",
    "        self.softmax = nn.Softmax(dim=1)  # dim = 1 -> 0 to 1\n",
    "\n",
    "    # Specifies data flow/forward pass\n",
    "    def forward(self, input_data):\n",
    "        flattened_data = self.flatten(input_data)\n",
    "        logits = self.dense_layers(flattened_data)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Checks if GPU (cuda) is available and use it. If not, use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "feed_forward_net = FeedForwardNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # Backpropagate loss and update weights\n",
    "        optimizer.zero_grad()  # Reset gradients for each batch\n",
    "        loss.backward()  # Backprop\n",
    "        optimiser.step()  # Update weights\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_one_epoch(model, data_loader, loss_fn, optimizer, device)\n",
    "        print(\"-----------------\")\n",
    "    print(\"Training is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate loss function + optimiser\n",
    "LEARNING_RATE = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(feed_forward_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "# If model state dict already exists, load it into the model\n",
    "if os.path.isfile(\"feedforwardnet.pth\") == True:\n",
    "    state_dict = torch.load(\"feedforwardnet.pth\")\n",
    "    feed_forward_net.load_state_dict(state_dict)\n",
    "else:\n",
    "    train(feed_forward_net, train_data_loader, loss_fn, optimizer, device, EPOCHS)\n",
    "    # Store model\n",
    "    torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n",
    "    print(\"Model trained and stored at feedforwardnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_, target, class_mapping):\n",
    "    model.eval() # For evaluation and inference, turns off certain layers like batch_normalization and dropout\n",
    "    # model.train() # Turns back on layers offed by eval (As we are only looking at inference, don't need this)\n",
    "    \n",
    "    # no need to calculate gradients as we are doing inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_.to(device)) # Tensor (1,10) -> [[0.1,0.1,...,0.6]] sum = 1 due to softmax\n",
    "        predicted_index = predictions[0].argmax(0) # Prediction for example [0]\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "        \n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: '7', Expected: '7'\n"
     ]
    }
   ],
   "source": [
    "_, validation_data = download_mnist_datasets()\n",
    "\n",
    "# Get sample from validation set\n",
    "# Input stored at [0][0] and output stored at [0][1] for example in [0]\n",
    "input_, target = (\n",
    "    validation_data[0][0],\n",
    "    validation_data[0][1],\n",
    ")\n",
    "\n",
    "# Make an inference\n",
    "predicted, expected = predict(feed_forward_net, input_, target, class_mapping)\n",
    "print(f\"Predicted: '{predicted}', Expected: '{expected}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a class and dataloader with Urban8K\n",
    "\n",
    "This is to preprocess all the audio so that they are in mono and have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_file,\n",
    "        audio_dir,\n",
    "        transformation,\n",
    "        target_sample_rate,\n",
    "        num_samples,\n",
    "        device,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)  # Use GPU\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        # get tensor of audio and the sample rate\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(\n",
    "            self.device\n",
    "        )  # Signal registered onto device, able to use GPU\n",
    "        signal = self._clean_signal(signal, sr)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    # Performs resampling, demixing and cutting/padding\n",
    "    def _clean_signal(self, signal, sr):\n",
    "        # signal.shape -> (num_channels, samples)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        # Signals might have different sample rates\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        # signal is tensor of (1, num_samples)\n",
    "        # if length of signal more than num_samples\n",
    "        # cut until num_samples\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, : self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        if signal.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            # 1st argument represents amount to prepend padding\n",
    "            # 2nd argument represents amount to append padding\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            # pad function starts padding from last dimension\n",
    "            # eg if pad(1, 1, 2, 2), pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    # Convert original sampling rate to a specified sampling rate\n",
    "    # single underscore methods for internal use\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(device) # Use GPU\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    # Convert to mono channel\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"  # see csv for locations\n",
    "        file_name = self.annotations.iloc[index, 0]\n",
    "        path = os.path.join(self.audio_dir, fold, file_name)\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]  # see csv for locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE= \"UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \"UrbanSound8K/audio\"\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "# Create callable object -> mel_spectrogram can be directly applied to the signal\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    ")\n",
    "\n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8732 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "\n",
    "signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 44])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape # (channels, mels, no of frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CNN for Sound Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(\n",
    "            128 * 5 * 4, 10\n",
    "        )  # out_channels * freq_axis * time_axis in this layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = CNNNetwork()\n",
    "summary(cnn.cuda(), (1, 64, 44))  # From signal.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see how the output shape changes over time.<br>\n",
    "In Conv2d-1 the input of **(1, 64, 44)** becomes **(16, 66, 46)**<br>\n",
    "Just before the flatten layer, we see that the input beomes **(128, 5, 4)**. This is where 128 * 5 * 4 is derived from to input into the dense (nn.Linear) layer.\n",
    "\n",
    "The next part will be implemented in a proper python application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data, batch_size):\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_data_loader\n",
    "\n",
    "\n",
    "def train_single_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, data_loader, loss_fn, optimizer, device)\n",
    "        print(\"-------------------\")\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "CNNNetwork(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Epoch 1\n",
      "loss: 2.3677124977111816\n",
      "-------------------\n",
      "Epoch 2\n",
      "loss: 2.1811063289642334\n",
      "-------------------\n",
      "Epoch 3\n",
      "loss: 2.1753122806549072\n",
      "-------------------\n",
      "Epoch 4\n",
      "loss: 2.1754343509674072\n",
      "-------------------\n",
      "Epoch 5\n",
      "loss: 2.175436496734619\n",
      "-------------------\n",
      "Epoch 6\n",
      "loss: 2.175436496734619\n",
      "-------------------\n",
      "Epoch 7\n",
      "loss: 2.174402952194214\n",
      "-------------------\n",
      "Epoch 8\n",
      "loss: 2.175436496734619\n",
      "-------------------\n",
      "Epoch 9\n",
      "loss: 2.175436496734619\n",
      "-------------------\n",
      "Epoch 10\n",
      "loss: 2.175436496734619\n",
      "-------------------\n",
      "Finished training\n",
      "Trained neural net saved at neuralnet.pth\n"
     ]
    }
   ],
   "source": [
    "# Checks if GPU (cuda) is available and use it. If not, use CPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Create callable object -> mel_spectrogram can be directly applied to the signal\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
    ")\n",
    "\n",
    "usd = UrbanSoundDataset(\n",
    "    ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device\n",
    ")\n",
    "\n",
    "train_data_loader = create_data_loader(usd, BATCH_SIZE)\n",
    "\n",
    "# construct model and assign to device\n",
    "cnn = CNNNetwork().to(device)\n",
    "print(cnn)\n",
    "\n",
    "# initialize loss function + optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train model\n",
    "train(cnn, train_data_loader, loss_fn, optimizer, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(cnn.state_dict(), \"neuralnet.pth\")\n",
    "print(\"Trained neural net saved at neuralnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 661.3 GB\n"
     ]
    }
   ],
   "source": [
    "torch.rand(20000,20000).cuda()\n",
    "print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/10243,1), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
